# IMT

## Manual quality checks

## What do google do?

[[IMT_Jēdzieni#Google|Google]] has a few thousand human reviewers that rate pages

### Page reputation

Google likes brands, as with that comes reliability that your results are what users expect.

### Topicality

The pages should stay on topic, if they don't [[IMT_Jēdzieni#Google|Google]] won't know how to rank them.
Off-topic links should have [[IMT_Jēdzieni#Nofollow-links|nofollow]] attributes.

User generated content, if noted as such shouldn't impact the topicality score much.

### Reviews

[[IMT_Jēdzieni#Google|Google]] reads reviews! Looking at the API leak, Google can demote results based on ratings from reviews.

## Text spam

To bump yourself up in search results you could have just a bunch of keywords hidden in your page. A search engine would read these and increase your rating.
[[IMT_Jēdzieni#Google|Google]] is fighting against this and will reduce your rating if you spam keywords.

## Reciprocal linking

[[IMT_Jēdzieni#Google|Google]] dislikes reciprocal links (link loops)

## Domain demotion

Unless you are a brand, your domain will be demoted if it's very close to the search query to prevent spam

## Expired domain abuse

Recently expired domains are demoted

## Context shapes relationships

If 2 items have similar contexts, we can assume they are both connected through this context. We can measure this context and we don't even have to know what that means.

## Word vectors

For each word, you can create an n-dimensional vector where each word has a vector value, similar vectors have similar meanings.

Word2Vec is a popular algorithm.

It does falter slightly when a word has multiple meanings.

BERT is an alternative. It builds contextual word vectors.

Sentence-BERT can generate a vector for whole sentences. It's pretty good at answering questions.

We can use these vectors to check for topicality.


## Optimize Individual pages for Individual keyword phrases

Pages shouldn't compete content-wise, so search engines have a better distinction and are considered less spammy.

## Keyword placement

The `<meta>` HTML tag can contain a useful description to show to people, usually ignored by search engines

## Long tail keywords

Very specific keywords, they are easy to rank.

## Consistent url

Keep subdomains consistent.

## Brands are most important

Modern search engines hold brand as most important, so you need to be high quality and make them trust you.

## Loading speed 

The page _should_ load in 2.5s, 2.5s-4s needs improvement, 4s is too long, search engines will demote them.

## Cumulative Layout Shift (CLS)

Search engines do measure how much the layout changes, as that raises user annoyance.

## Accessibility

First of all that means mobile friendly.
CLS contributes to that.

## Unigram Language model

If we have a word set, each word probability is n/N where n is occurances and N is the set size.

Probability of multiple words is the probabilities multiplied.

P(a) = x
P(b) = y

P(a b) = x\*y

Text similarity with this model:
Larger probability has better text similarity

## Guessing probabilities

Tuning param $0\le\lambda\le1$

$P(w|D)=\lambda P(w|\theta_D)+(1-\lambda)P(w|\theta_C)$

This guess is based on a known probability of a different part of a text.


$$\textit{sim}(Q,D) = \prod_{i=1}^{n}{\lambda\frac{\text{tf}_D(q_i)}{N_D}}+
(1-\lambda)\frac{\text{tf}_C(q_i)}{N_C}
$$

$C$ - the whole collection
$D$ - The document
$\text{tf}_X(w)$ - term frequency of word $w$ in $X$
$\lambda$ - tuning parameter
$q_i$ - $i$-th term in the query